{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines = 506\n",
    "n_cols = 13\n",
    "data = torch.tensor(())\n",
    "data = data.new_zeros((n_lines,n_cols))\n",
    "labels = torch.tensor(())\n",
    "labels = labels.new_zeros((n_lines))\n",
    "f = open('housing.csv')\n",
    "n_line = 0\n",
    "for line in f:\n",
    "    cols = line.split()\n",
    "    for i in range(n_cols):\n",
    "        data[n_line][i] = float(cols[i])\n",
    "    labels[n_line] = float( int( float(cols[n_cols]) ) )\n",
    "    #labels[n_line] = float( int( float(cols[n_cols]) / 5) )  # цена с точностью до 5к долларов\n",
    "    n_line += 1\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# варим фичи :)\n",
    "# нормализуем параметры data\n",
    "n_train_lines = 400\n",
    "data_norm = torch.tensor(())\n",
    "data_norm = data_norm.new_zeros((n_lines,n_cols))\n",
    "data_min_max = torch.tensor(())\n",
    "data_min_max = data_min_max.new_zeros((n_cols, 3))\n",
    "\n",
    "# fuck the pandas\n",
    "# find min and max\n",
    "for i in range(n_cols):\n",
    "    data_min_max[i][0] = 1000000  # set min\n",
    "    data_min_max[i][1] = -1000000   # set max \n",
    "\n",
    "for i in range(n_lines):\n",
    "    for j in range(n_cols):\n",
    "        if data[i][j] < data_min_max[j][0]:\n",
    "            data_min_max[j][0] = data[i][j]\n",
    "        if data[i][j] > data_min_max[j][1]:\n",
    "            data_min_max[j][1] = data[i][j]    \n",
    "\n",
    "# calculate disperse\n",
    "for i in range(n_cols):\n",
    "    data_min_max[i][2] = data_min_max[i][1] - data_min_max[i][0]\n",
    "    \n",
    "# nomralize data, значения 0..1\n",
    "for i in range(n_lines):\n",
    "    for j in range(n_cols):\n",
    "        data_norm[i][j] = (data[i][j] - data_min_max[j][0]) / data_min_max[j][2] * 1\n",
    "\n",
    "data_train = torch.tensor(())\n",
    "data_train = data_train.new_zeros((n_train_lines,n_cols))\n",
    "labels_train = torch.tensor(())\n",
    "labels_train = labels_train.new_zeros((n_train_lines))\n",
    "\n",
    "data_train = data_norm[:n_train_lines]\n",
    "labels_train = labels[:n_train_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = torch.tensor(())\n",
    "data_test = data_test.new_zeros((n_lines-n_train_lines,n_cols))\n",
    "labels_test = torch.tensor(())\n",
    "labels_test = labels_test.new_zeros((n_lines-n_train_lines))\n",
    "\n",
    "data_test = data_norm[n_train_lines:]\n",
    "labels_test = labels[n_train_lines:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = TensorDataset(data_train, labels_train)\n",
    "data_iter = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.nn.Sequential(torch.nn.Linear(n_cols, 1))\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(n_cols, 5), \n",
    "    #torch.nn.Sigmoid(),\n",
    "    torch.nn.ReLU(True),\n",
    "    #torch.nn.BatchNorm1d(5),\n",
    "    torch.nn.Linear(5, 1),\n",
    ")\n",
    "loss = torch.nn.MSELoss(reduction='mean')\n",
    "#trainer = torch.optim.SGD(model.parameters(), lr=0.00001,momentum=0.9,nesterov=True)\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641-253 624-243 603-229 571-209 522-179 451-136 357-84 257-42 177-26 129-37 106-58 97-76 94-86 91-91 90-91 88-90 87-87 85-85 84-82 83-79 81-79 80-75 79-72 78-69 77-66 75-63 74-61 73-58 72-57 71-56 70-52 69-49 68-48 68-46 67-44 66-43 65-41 64-41 64-39 63-37 62-36 61-34 61-33 60-32 59-31 59-30 58-29 57-28 57-28 56-27 56-26 55-25 55-25 54-24 54-24 53-24 53-23 52-23 52-22 51-22 51-21 50-21 50-21 50-21 49-20 49-20 48-20 48-20 48-20 47-19 47-19 47-19 46-19 46-19 46-19 45-19 45-19 45-19 44-19 44-19 44-19 44-19 43-19 43-19 43-19 42-19 42-19 42-19 42-19 41-19 41-19 41-19 41-19 40-19 40-19 40-19 40-19 39-19 39-19 39-19 39-19 38-19 38-19 38-19 38-19 38-19 37-19 37-19 37-19 37-19 36-19 36-19 36-19 36-19 36-19 36-19 35-19 35-19 35-19 35-19 35-19 34-19 34-19 34-19 34-19 34-19 34-19 33-19 33-19 33-19 33-19 33-19 33-19 33-19 32-19 32-19 32-19 32-19 32-19 32-19 32-19 31-19 31-18 31-18 31-18 31-18 31-18 31-18 31-18 30-18 30-18 30-18 30-18 30-18 30-18 30-18 30-18 30-18 30-18 29-18 29-18 29-18 29-18 29-18 29-18 29-18 29-18 29-18 29-18 29-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 28-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 27-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-18 26-19 26-18 25-18 25-18 25-18 25-19 25-19 25-19 25-19 25-19 25-18 25-18 25-18 25-18 25-19 25-19 25-18 25-18 25-19 25-18 25-19 25-19 25-19 25-19 25-19 25-18 25-18 25-18 25-19 25-19 25-19 25-18 25-19 25-18 25-19 25-18 24-19 24-18 24-19 24-18 24-19 24-19 24-18 24-18 24-19 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 24-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-17 23-18 23-17 23-17 23-18 23-18 23-17 23-17 23-17 23-17 23-17 23-17 23-17 23-18 23-18 23-17 23-17 23-17 23-18 23-17 23-17 23-17 23-17 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 23-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-18 22-19 22-18 22-18 22-18 22-18 22-19 22-18 22-18 22-18 22-18 22-18 22-19 22-18 22-18 22-18 22-18 22-19 22-18 22-19 22-19 22-19 22-19 22-18 22-19 22-19 22-19 22-18 22-18 22-18 22-18 22-18 22-18 22-19 22-18 22-18 22-18 22-18 22-18 22-19 22-18 22-19 22-19 22-19 22-18 22-18 22-18 22-18 22-18 22-18 22-19 22-18 22-19 22-19 22-19 22-19 22-18 22-19 22-19 22-19 22-19 22-19 22-18 22-18 21-19 21-19 21-19 21-19 21-19 21-18 21-19 21-18 21-19 21-19 21-19 21-18 21-19 21-19 21-19 21-18 21-18 21-18 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-18 21-19 21-19 21-18 21-19 21-18 21-19 21-18 21-18 21-18 21-19 21-19 21-19 21-19 21-19 21-19 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-18 21-19 21-18 21-18 20-18 20-19 20-19 20-19 20-18 20-18 20-18 20-19 20-19 20-19 20-18 20-18 20-19 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-19 20-19 20-19 20-18 20-18 20-18 20-19 20-19 20-18 20-18 20-18 20-18 20-18 20-19 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-19 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-19 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-19 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 20-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-17 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-17 19-18 19-17 19-18 19-18 19-18 19-18 19-19 19-18 19-18 19-18 19-18 19-19 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 19-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-17 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-17 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-17 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-19 18-18 18-19 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-19 18-18 18-19 18-18 18-18 18-18 18-19 18-18 18-18 18-18 18-18 18-18 18-18 18-19 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-19 18-19 18-19 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-18 18-19 18-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-17 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-19 17-18 17-18 17-18 17-18 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-17 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-18 17-19 17-18 17-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-18 16-19 16-18 16-18 16-19 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-19 16-19 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-17 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-17 16-18 16-17 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-18 16-17 16-18 16-18 16-18 16-18 16-18 16-19 16-18 16-18 16-18 16-18 16-18 16-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-17 15-17 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-19 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-17 15-18 15-18 15-17 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-17 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-17 15-17 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-19 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-17 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-17 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-17 15-18 15-17 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 15-18 \n",
      "Train loss: 15.108776092529297\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(0, num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        #print (X, y)\n",
    "        trainer.zero_grad()\n",
    "        l = loss(model.forward(X).reshape(-1), y)\n",
    "        #print (f'loss={l}')\n",
    "        #for par in model.parameters():\n",
    "        #    print (par)\n",
    "        #    break\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    l = loss(model.forward(data_train).reshape(-1), labels_train)\n",
    "    ltest = loss(model(data_test).reshape(-1), labels_test)\n",
    "    print (f'{int(l)}-{int(ltest)}', end=\" \")\n",
    "    #print('epoch %d, loss: %i, loss test: %i' % (epoch, l.item(), ltest.item()))\n",
    "    #print('epoch %d, loss: %i, loss test: ' % (epoch, l.item()))\n",
    "    #print('%i' % (l.item()), end=' ')\n",
    "    #print('w', model[0].weight.data)\n",
    "    #print('b', model[0].bias.data)\n",
    "    \n",
    "print (f'\\nTrain loss: {loss(model(data_train).reshape(-1), labels_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss = 18.67812728881836\n",
      "\n",
      "tensor([11.4072], grad_fn=<AddBackward0>) tensor(5.)\n",
      "tensor([15.6897], grad_fn=<AddBackward0>) tensor(7.)\n",
      "tensor([16.1100], grad_fn=<AddBackward0>) tensor(12.)\n",
      "tensor([12.9494], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([8.2331], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([7.8547], grad_fn=<AddBackward0>) tensor(5.)\n",
      "tensor([10.2125], grad_fn=<AddBackward0>) tensor(11.)\n",
      "tensor([21.5754], grad_fn=<AddBackward0>) tensor(27.)\n",
      "tensor([13.1464], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([19.4769], grad_fn=<AddBackward0>) tensor(27.)\n",
      "tensor([19.7605], grad_fn=<AddBackward0>) tensor(15.)\n",
      "tensor([16.0738], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([5.0521], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([11.7404], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([2.0156], grad_fn=<AddBackward0>) tensor(7.)\n",
      "tensor([9.5224], grad_fn=<AddBackward0>) tensor(7.)\n",
      "tensor([12.1952], grad_fn=<AddBackward0>) tensor(7.)\n",
      "tensor([8.4436], grad_fn=<AddBackward0>) tensor(10.)\n",
      "tensor([6.0735], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([13.2666], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([20.3441], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([17.5355], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([18.8968], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([12.7242], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([14.8736], grad_fn=<AddBackward0>) tensor(11.)\n",
      "tensor([10.4372], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([18.1559], grad_fn=<AddBackward0>) tensor(10.)\n",
      "tensor([16.0168], grad_fn=<AddBackward0>) tensor(10.)\n",
      "tensor([13.6669], grad_fn=<AddBackward0>) tensor(11.)\n",
      "tensor([12.4432], grad_fn=<AddBackward0>) tensor(9.)\n",
      "tensor([19.2479], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([19.1709], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([27.3545], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([18.7184], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([17.5718], grad_fn=<AddBackward0>) tensor(11.)\n",
      "tensor([12.7011], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([14.8732], grad_fn=<AddBackward0>) tensor(9.)\n",
      "tensor([9.5873], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([6.9470], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([13.3669], grad_fn=<AddBackward0>) tensor(12.)\n",
      "tensor([12.6304], grad_fn=<AddBackward0>) tensor(10.)\n",
      "tensor([15.8090], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([17.2214], grad_fn=<AddBackward0>) tensor(18.)\n",
      "tensor([16.2250], grad_fn=<AddBackward0>) tensor(15.)\n",
      "tensor([11.9386], grad_fn=<AddBackward0>) tensor(10.)\n",
      "tensor([11.7893], grad_fn=<AddBackward0>) tensor(11.)\n",
      "tensor([16.2570], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([16.6467], grad_fn=<AddBackward0>) tensor(12.)\n",
      "tensor([16.1956], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([15.7583], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([18.6803], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([18.3360], grad_fn=<AddBackward0>) tensor(15.)\n",
      "tensor([17.0722], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([23.6465], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([16.0819], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([16.5373], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([13.2001], grad_fn=<AddBackward0>) tensor(12.)\n",
      "tensor([13.4430], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([16.8886], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([18.0298], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([19.8977], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([20.6892], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([20.4418], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([26.6341], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([21.5440], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([18.0015], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([14.9381], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([15.7621], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([15.9098], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([17.5666], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([19.2835], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([25.2967], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([23.9398], grad_fn=<AddBackward0>) tensor(23.)\n",
      "tensor([30.6516], grad_fn=<AddBackward0>) tensor(29.)\n",
      "tensor([15.7760], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([14.7448], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([18.5236], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([12.0767], grad_fn=<AddBackward0>) tensor(12.)\n",
      "tensor([17.0082], grad_fn=<AddBackward0>) tensor(14.)\n",
      "tensor([23.5984], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([27.3771], grad_fn=<AddBackward0>) tensor(23.)\n",
      "tensor([34.7446], grad_fn=<AddBackward0>) tensor(23.)\n",
      "tensor([37.5055], grad_fn=<AddBackward0>) tensor(25.)\n",
      "tensor([24.0490], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([20.3645], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([26.1240], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([19.4722], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([23.8202], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([12.7059], grad_fn=<AddBackward0>) tensor(15.)\n",
      "tensor([9.9961], grad_fn=<AddBackward0>) tensor(7.)\n",
      "tensor([6.9881], grad_fn=<AddBackward0>) tensor(8.)\n",
      "tensor([13.5684], grad_fn=<AddBackward0>) tensor(13.)\n",
      "tensor([15.3925], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([19.5489], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([19.3860], grad_fn=<AddBackward0>) tensor(24.)\n",
      "tensor([17.1954], grad_fn=<AddBackward0>) tensor(23.)\n",
      "tensor([15.0808], grad_fn=<AddBackward0>) tensor(19.)\n",
      "tensor([18.5019], grad_fn=<AddBackward0>) tensor(18.)\n",
      "tensor([19.6743], grad_fn=<AddBackward0>) tensor(21.)\n",
      "tensor([17.9729], grad_fn=<AddBackward0>) tensor(17.)\n",
      "tensor([19.0118], grad_fn=<AddBackward0>) tensor(16.)\n",
      "tensor([21.7746], grad_fn=<AddBackward0>) tensor(22.)\n",
      "tensor([21.2487], grad_fn=<AddBackward0>) tensor(20.)\n",
      "tensor([26.3895], grad_fn=<AddBackward0>) tensor(23.)\n",
      "tensor([23.7029], grad_fn=<AddBackward0>) tensor(22.)\n",
      "tensor([21.4039], grad_fn=<AddBackward0>) tensor(11.)\n"
     ]
    }
   ],
   "source": [
    "#test model\n",
    "\n",
    "\n",
    "l = loss(model(data_test).reshape(-1), labels_test)\n",
    "print (f'Test loss = {l}\\n')\n",
    "\n",
    "for i in range(n_lines-n_train_lines):\n",
    "    dat = data_test[i]\n",
    "    print (model(dat), labels_test[i])\n",
    "\n",
    "# batch_size = 1\n",
    "# dataset_test = TensorDataset(data_test, labels_test)\n",
    "# data_iter_test = DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "# for X, y in data_iter:\n",
    "#     print (X,y)\n",
    "#     #pp = model.forward(X).reshape(-1)\n",
    "#     l = loss(model.forward(X).reshape(-1), y)\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
