{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = pd.read_csv('train.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twits[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99989/99989 [12:26<00:00, 134.02it/s]\n"
     ]
    }
   ],
   "source": [
    "todelete = []\n",
    "clines = twits.shape[0]\n",
    "# clean twits\n",
    "for nline in tqdm(range(clines)):\n",
    "    line = twits.iloc[nline,2]\n",
    "    line = line.lower()\n",
    "    \n",
    "    # remove &xx\n",
    "    line = re.sub(r'&\\S*', '', line)\n",
    "    # change all @nicks to @twuser\n",
    "    line = re.sub(r'@\\S*', '@twuser', line)\n",
    "    # change all ......... to ...\n",
    "    line = re.sub(r'\\.+', ' ...', line)\n",
    "    # remove all . , : ; ( ) after text\n",
    "    line = re.sub(r'(\\S)[\\.\\,\\:\\;\\)\\(]', r'\\1', line)\n",
    "    # remove all . , : ; ( ) before text\n",
    "    line = re.sub(r'[\\.\\,\\:\\;\\)\\(](\\S)', r'\\1', line)\n",
    "    # remove -*\n",
    "    line = re.sub(r'-\\*', '', line)\n",
    "    # change ))))) -> ) и (((( -> (\n",
    "    line = re.sub(r'\\)+', '\\)', line)\n",
    "    line = re.sub(r'\\(+', '\\(', line)\n",
    "    # нафик фигню\n",
    "    line = re.sub(r'\\\\\\\\[\\(\\)]', '', line)\n",
    "    line = re.sub(r'(\\S)\\- ', r'\\1 ', line)\n",
    "    line = re.sub(r'(\\S)\\- ', r'\\1 ', line)\n",
    "    #line = re.sub(r'[^a-z 0-9\\,\\.\\=\\-\\(\\)\\[\\]\\:\\;\\'\\+]', '', line)    \n",
    "    # убрать ссылки\n",
    "    line = re.sub(r'http:\\S*', '', line)\n",
    "    line = re.sub(r'www.\\S*', '', line)\n",
    "    # убрать много минусов, воскл, вопр знаков\n",
    "    line = re.sub(r'[\\-]+', ' - ', line)\n",
    "    line = re.sub(r'[\\!]+', ' ! ', line)\n",
    "    line = re.sub(r'[\\?]+', ' ? ', line)\n",
    "    # заменить 3 и более символов на один\n",
    "    line = re.sub(r'([\\S])\\1{3,}', r'\\1', line)\n",
    "    # фигня в начале\n",
    "    line = re.sub(r'(^[^\\w]+)', '', line)\n",
    "    # 2 пробела -> один\n",
    "    line = re.sub(r' +', ' ', line)\n",
    "    \n",
    "#     line = re.sub(r'', '', line)\n",
    "#     line = re.sub(r'', '', line)\n",
    "#     line = re.sub(r'', '', line)\n",
    "#     line = re.sub(r'', '', line)\n",
    "#     line = re.sub(r'', '', line)\n",
    "#     line = re.sub(r'', '', line)\n",
    "    \n",
    "    line = line.strip()\n",
    "    \n",
    "    if len(line) > 0:\n",
    "        twits.iloc[nline,2] = line\n",
    "    else:\n",
    "        todelete.append(nline)\n",
    "    \n",
    "for i in sorted(todelete, reverse = True):\n",
    "    twits.drop(twits.index[[i]], inplace = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this - 22'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \"this р°р»сњс-рѕрµс† 22\"\n",
    "re.sub(r'[^a-z 0-9\\,\\.\\=\\-\\(\\)\\[\\]\\:\\;\\'\\+]', '', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'really not; fun. ) go'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = \",really not; fun. :) go\"\n",
    "re.sub(r'[\\.\\,\\:\\;](\\S)', r'\\1', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits.to_csv(\"train_amended2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "twits = pd.read_csv('train_amended2.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twits 2 lists\n",
    "clines = twits.shape[0]\n",
    "phrases = []\n",
    "tonality = torch.zeros(clines, dtype=torch.int64).to(dev)\n",
    "\n",
    "\n",
    "for nline in range(clines):\n",
    "    phrases.append(twits.iloc[nline,2])\n",
    "    tonality[nline] = torch.tensor(twits.iloc[nline,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(twits.iloc[nline,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1,  ..., 0, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tonality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(phrases)\n",
    "ALL_WORDS = set(text.strip().split(' '))\n",
    "len(ALL_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twuser', 83256),\n",
       " ('.', 64374),\n",
       " ('i', 49306),\n",
       " ('!', 42278),\n",
       " ('the', 29715),\n",
       " ('to', 29652),\n",
       " ('you', 26942),\n",
       " ('a', 22917),\n",
       " ('it', 17430),\n",
       " ('?', 17154)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Counter(text.split(' '))\n",
    "c.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63600"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем слова с числом повторений меньше 3\n",
    "todelete = set()\n",
    "for i, n in c.items():\n",
    "    if n < 3:\n",
    "        todelete.add(i)\n",
    "        \n",
    "for i in todelete:\n",
    "    del c[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16034"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99984, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16036"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS_COUNT = len(ALL_WORDS)\n",
    "ALL_WORDS = set([w for w, _ in c.most_common(WORDS_COUNT)])\n",
    "INDEX_TO_WORD = ['<pad>', '<miss>'] + list(ALL_WORDS)\n",
    "len(INDEX_TO_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<miss>',\n",
       " 'slipped',\n",
       " 'house',\n",
       " 'slr',\n",
       " 'chilly',\n",
       " '#samesexsunday',\n",
       " 'that',\n",
       " '.both',\n",
       " 'suggested']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORD_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_WORD)}\n",
    "INDEX_TO_WORD[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.8970e+03, 1.2296e+04, 9.6800e+03, 1.4618e+04, 1.3057e+04,\n",
       "        7.6980e+03, 1.0378e+04, 9.5070e+03, 6.1200e+03, 7.8060e+03,\n",
       "        4.0120e+03, 7.0500e+02, 1.6900e+02, 2.1000e+01, 5.0000e+00,\n",
       "        5.0000e+00, 6.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  1.  ,   3.66,   6.32,   8.98,  11.64,  14.3 ,  16.96,  19.62,\n",
       "         22.28,  24.94,  27.6 ,  30.26,  32.92,  35.58,  38.24,  40.9 ,\n",
       "         43.56,  46.22,  48.88,  51.54,  54.2 ,  56.86,  59.52,  62.18,\n",
       "         64.84,  67.5 ,  70.16,  72.82,  75.48,  78.14,  80.8 ,  83.46,\n",
       "         86.12,  88.78,  91.44,  94.1 ,  96.76,  99.42, 102.08, 104.74,\n",
       "        107.4 , 110.06, 112.72, 115.38, 118.04, 120.7 , 123.36, 126.02,\n",
       "        128.68, 131.34, 134.  ]),\n",
       " <a list of 50 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# распреление длины слов в строке\n",
    "plt.hist([len(s.split(' ')) for s in phrases], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((len(phrases), MAX_LEN), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99984/99984 [00:14<00:00, 6707.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# готовим матрицу - строка - фраза, столбец - индекс слова в этой позиции\n",
    "for i in tqdm(range(len(phrases))):\n",
    "    for j, w in enumerate(phrases[i].split(' ')):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = WORD_TO_INDEX.get(w, WORD_TO_INDEX['<miss>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15365,  9350,  2552,  3596, 14642,     1,  9694, 10420,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99984, 35])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99984"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tonality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "split = 0.2\n",
    "clines = X.shape[0]\n",
    "ctrainlines = int( clines * (1-split) )\n",
    "X_train = X[0:ctrainlines].to(dev)\n",
    "X_test = X[ctrainlines:].to(dev)\n",
    "Y_train = tonality[0:ctrainlines].to(dev)\n",
    "Y_test = tonality[ctrainlines:].to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dev, batch_size):\n",
    "        super(Network, self).__init__()\n",
    "        self.dev = dev\n",
    "        self.input_size = len(INDEX_TO_WORD)\n",
    "        self.embed_size = 200\n",
    "        self.hidden_size = 128\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = torch.nn.Embedding(self.input_size, self.embed_size).to(self.dev)\n",
    "        self.gru = torch.nn.RNN(self.embed_size, self.hidden_size, num_layers = self.n_layers, nonlinearity = 'relu', batch_first=True).to(self.dev)\n",
    "        self.hidden2tag = torch.nn.Linear(self.hidden_size*34, 2).to(self.dev)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=2).to(self.dev)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.n_layers*2, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.n_layers*2, batch_size, self.hidden_size)\n",
    "        return h0, c0        \n",
    "\n",
    "    def forward(self, sentences):\n",
    "        #self.hidden = self.init_hidden(batch_size)\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, hidden = self.gru(embeds)\n",
    "        gru_out = gru_out.reshape(self.batch_size,1,-1)\n",
    "        #tag_space = self.hidden2tag(gru_out)\n",
    "        gru_out = self.hidden2tag(gru_out)\n",
    "        #gru_out = self.softmax(gru_out)\n",
    "        return gru_out, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkGRU(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dev, batch_size):\n",
    "        super(NetworkGRU, self).__init__()\n",
    "        self.dev = dev\n",
    "        self.input_size = len(INDEX_TO_WORD)\n",
    "        self.embed_size = 200\n",
    "        self.hidden_size = 128\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = torch.nn.Embedding(self.input_size, self.embed_size).to(self.dev)\n",
    "        self.gru = torch.nn.GRU(self.embed_size, self.hidden_size, num_layers = self.n_layers, batch_first=True).to(self.dev)\n",
    "        self.hidden2tag = torch.nn.Linear(self.hidden_size*34, 2).to(self.dev)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, hidden = self.gru(embeds)\n",
    "        gru_out = gru_out.reshape(self.batch_size,1,-1)\n",
    "        gru_out = self.hidden2tag(gru_out)\n",
    "        return gru_out, hidden\n",
    "\n",
    "\n",
    "    def forward_state(self, sentences, state):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds, state)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLSTM(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dev, batch_size):\n",
    "        super(NetworkLSTM, self).__init__()\n",
    "        self.dev = dev\n",
    "        self.input_size = len(INDEX_TO_WORD)\n",
    "        self.embed_size = 200\n",
    "        self.hidden_size = 128\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = torch.nn.Embedding(self.input_size, self.embed_size).to(self.dev)\n",
    "        self.gru = torch.nn.LSTM(self.embed_size, self.hidden_size, num_layers = self.n_layers, batch_first=True).to(self.dev)\n",
    "        self.hidden2tag = torch.nn.Linear(self.hidden_size*34, 2).to(self.dev)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, hidden = self.gru(embeds)\n",
    "        gru_out = gru_out.reshape(self.batch_size,1,-1)\n",
    "        gru_out = self.hidden2tag(gru_out)\n",
    "        return gru_out, hidden\n",
    "\n",
    "    def forward_state(self, sentences, state):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds, state)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 35])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16036"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDEX_TO_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100\n",
    "model = Network(dev,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGRU = NetworkGRU(dev,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLSTM = NetworkLSTM(dev,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrate = 0.1\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lrate)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=lrate)\n",
    "\n",
    "criterionGRU = torch.nn.CrossEntropyLoss()\n",
    "optimizerGRU = torch.optim.SGD(modelGRU.parameters(), lr=lrate)\n",
    "\n",
    "criterionLSTM = torch.nn.CrossEntropyLoss()\n",
    "optimizerLSTM = torch.optim.SGD(modelLSTM.parameters(), lr=lrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time,GRU,LSTM: 5.949,6.498,6.981, Train loss,GRU,LSTM: 0.656,1.512,0.604, delta RGU,LSTM:-0.856,0.052\n",
      "Epoch 1. Time,GRU,LSTM: 5.743,6.485,7.006, Train loss,GRU,LSTM: 0.521,0.552,0.537, delta RGU,LSTM:-0.031,-0.016\n",
      "Epoch 2. Time,GRU,LSTM: 5.724,6.486,6.998, Train loss,GRU,LSTM: 0.488,0.504,0.502, delta RGU,LSTM:-0.016,-0.014\n",
      "Epoch 3. Time,GRU,LSTM: 5.745,6.486,7.001, Train loss,GRU,LSTM: 0.464,0.475,0.475, delta RGU,LSTM:-0.011,-0.011\n",
      "Epoch 4. Time,GRU,LSTM: 5.742,6.497,7.010, Train loss,GRU,LSTM: 0.444,0.452,0.453, delta RGU,LSTM:-0.008,-0.009\n",
      "Epoch 5. Time,GRU,LSTM: 5.741,6.501,7.012, Train loss,GRU,LSTM: 0.427,0.431,0.434, delta RGU,LSTM:-0.004,-0.007\n",
      "Epoch 6. Time,GRU,LSTM: 5.750,6.507,7.016, Train loss,GRU,LSTM: 0.411,0.410,0.416, delta RGU,LSTM:0.000,-0.005\n",
      "Epoch 7. Time,GRU,LSTM: 5.767,6.515,7.016, Train loss,GRU,LSTM: 0.395,0.390,0.397, delta RGU,LSTM:0.005,-0.003\n",
      "Epoch 8. Time,GRU,LSTM: 5.747,6.505,7.007, Train loss,GRU,LSTM: 0.379,0.369,0.379, delta RGU,LSTM:0.011,0.000\n",
      "Epoch 9. Time,GRU,LSTM: 5.751,6.513,7.022, Train loss,GRU,LSTM: 0.364,0.347,0.360, delta RGU,LSTM:0.017,0.003\n",
      "Epoch 10. Time,GRU,LSTM: 5.751,6.513,7.026, Train loss,GRU,LSTM: 0.349,0.324,0.341, delta RGU,LSTM:0.025,0.008\n",
      "Epoch 11. Time,GRU,LSTM: 5.774,6.524,7.033, Train loss,GRU,LSTM: 0.334,0.300,0.321, delta RGU,LSTM:0.035,0.013\n",
      "Epoch 12. Time,GRU,LSTM: 5.746,6.571,7.032, Train loss,GRU,LSTM: 0.319,0.275,0.301, delta RGU,LSTM:0.044,0.019\n",
      "Epoch 13. Time,GRU,LSTM: 5.743,6.530,7.034, Train loss,GRU,LSTM: 0.310,0.250,0.279, delta RGU,LSTM:0.060,0.031\n",
      "Epoch 14. Time,GRU,LSTM: 5.750,6.521,7.029, Train loss,GRU,LSTM: 0.298,0.225,0.258, delta RGU,LSTM:0.073,0.040\n",
      "Epoch 15. Time,GRU,LSTM: 5.732,6.526,7.041, Train loss,GRU,LSTM: 0.288,0.201,0.236, delta RGU,LSTM:0.087,0.052\n",
      "Epoch 16. Time,GRU,LSTM: 5.749,6.525,7.050, Train loss,GRU,LSTM: 0.276,0.178,0.214, delta RGU,LSTM:0.098,0.062\n",
      "Epoch 17. Time,GRU,LSTM: 5.741,6.519,7.047, Train loss,GRU,LSTM: 0.266,0.157,0.191, delta RGU,LSTM:0.109,0.075\n",
      "Epoch 18. Time,GRU,LSTM: 5.752,6.528,7.043, Train loss,GRU,LSTM: 0.255,0.141,0.170, delta RGU,LSTM:0.115,0.086\n",
      "Epoch 19. Time,GRU,LSTM: 5.768,6.543,7.052, Train loss,GRU,LSTM: 0.247,0.129,0.149, delta RGU,LSTM:0.118,0.098\n",
      "Epoch 20. Time,GRU,LSTM: 5.739,6.529,7.049, Train loss,GRU,LSTM: 0.241,0.117,0.129, delta RGU,LSTM:0.123,0.112\n",
      "Epoch 21. Time,GRU,LSTM: 5.751,6.536,7.055, Train loss,GRU,LSTM: 0.222,0.103,0.111, delta RGU,LSTM:0.119,0.111\n",
      "Epoch 22. Time,GRU,LSTM: 5.752,6.539,7.056, Train loss,GRU,LSTM: 0.214,0.092,0.097, delta RGU,LSTM:0.122,0.117\n",
      "Epoch 23. Time,GRU,LSTM: 5.734,6.525,7.042, Train loss,GRU,LSTM: 0.204,0.083,0.087, delta RGU,LSTM:0.121,0.117\n",
      "Epoch 24. Time,GRU,LSTM: 5.742,6.520,7.040, Train loss,GRU,LSTM: 0.199,0.077,0.083, delta RGU,LSTM:0.122,0.115\n",
      "Epoch 25. Time,GRU,LSTM: 5.766,6.519,7.046, Train loss,GRU,LSTM: 0.194,0.066,0.068, delta RGU,LSTM:0.128,0.126\n",
      "Epoch 26. Time,GRU,LSTM: 5.775,6.519,7.041, Train loss,GRU,LSTM: 0.177,0.058,0.055, delta RGU,LSTM:0.119,0.122\n",
      "Epoch 27. Time,GRU,LSTM: 5.742,6.525,7.049, Train loss,GRU,LSTM: 0.170,0.049,0.046, delta RGU,LSTM:0.121,0.124\n",
      "Epoch 28. Time,GRU,LSTM: 5.755,6.538,7.046, Train loss,GRU,LSTM: 0.166,0.043,0.040, delta RGU,LSTM:0.122,0.126\n",
      "Epoch 29. Time,GRU,LSTM: 5.771,6.537,7.039, Train loss,GRU,LSTM: 0.196,0.038,0.035, delta RGU,LSTM:0.158,0.161\n",
      "Epoch 30. Time,GRU,LSTM: 5.763,6.545,7.057, Train loss,GRU,LSTM: 0.196,0.034,0.031, delta RGU,LSTM:0.162,0.165\n",
      "Epoch 31. Time,GRU,LSTM: 5.750,6.537,7.049, Train loss,GRU,LSTM: 0.181,0.029,0.028, delta RGU,LSTM:0.153,0.153\n",
      "Epoch 32. Time,GRU,LSTM: 5.743,6.534,7.037, Train loss,GRU,LSTM: 0.154,0.025,0.025, delta RGU,LSTM:0.129,0.128\n",
      "Epoch 33. Time,GRU,LSTM: 5.731,6.526,7.030, Train loss,GRU,LSTM: 0.134,0.022,0.023, delta RGU,LSTM:0.112,0.111\n",
      "Epoch 34. Time,GRU,LSTM: 5.752,6.522,7.049, Train loss,GRU,LSTM: 0.120,0.020,0.022, delta RGU,LSTM:0.100,0.098\n",
      "Epoch 35. Time,GRU,LSTM: 5.740,6.519,7.041, Train loss,GRU,LSTM: 0.110,0.019,0.020, delta RGU,LSTM:0.091,0.090\n",
      "Epoch 36. Time,GRU,LSTM: 5.753,6.530,7.041, Train loss,GRU,LSTM: 0.107,0.018,0.019, delta RGU,LSTM:0.088,0.088\n",
      "Epoch 37. Time,GRU,LSTM: 5.742,6.529,7.048, Train loss,GRU,LSTM: 0.106,0.017,0.018, delta RGU,LSTM:0.089,0.088\n",
      "Epoch 38. Time,GRU,LSTM: 5.748,6.528,7.041, Train loss,GRU,LSTM: 0.106,0.017,0.017, delta RGU,LSTM:0.089,0.089\n",
      "Epoch 39. Time,GRU,LSTM: 5.749,6.527,7.022, Train loss,GRU,LSTM: 0.119,0.017,0.016, delta RGU,LSTM:0.102,0.102\n",
      "Epoch 40. Time,GRU,LSTM: 5.745,6.530,7.049, Train loss,GRU,LSTM: 0.144,0.016,0.016, delta RGU,LSTM:0.128,0.128\n",
      "Epoch 41. Time,GRU,LSTM: 5.808,6.526,7.052, Train loss,GRU,LSTM: 0.159,0.016,0.015, delta RGU,LSTM:0.143,0.144\n",
      "Epoch 42. Time,GRU,LSTM: 5.740,6.530,7.060, Train loss,GRU,LSTM: 0.147,0.016,0.015, delta RGU,LSTM:0.131,0.132\n",
      "Epoch 43. Time,GRU,LSTM: 5.742,6.533,7.044, Train loss,GRU,LSTM: 0.126,0.015,0.014, delta RGU,LSTM:0.111,0.112\n",
      "Epoch 44. Time,GRU,LSTM: 5.729,6.543,7.056, Train loss,GRU,LSTM: 0.118,0.015,0.014, delta RGU,LSTM:0.103,0.104\n",
      "Epoch 45. Time,GRU,LSTM: 5.763,6.520,7.040, Train loss,GRU,LSTM: 0.111,0.015,0.014, delta RGU,LSTM:0.096,0.098\n",
      "Epoch 46. Time,GRU,LSTM: 5.752,6.537,7.046, Train loss,GRU,LSTM: 0.104,0.015,0.013, delta RGU,LSTM:0.089,0.090\n",
      "Epoch 47. Time,GRU,LSTM: 5.738,6.531,7.050, Train loss,GRU,LSTM: 0.100,0.015,0.013, delta RGU,LSTM:0.085,0.087\n",
      "Epoch 48. Time,GRU,LSTM: 5.755,6.532,7.045, Train loss,GRU,LSTM: 0.099,0.014,0.013, delta RGU,LSTM:0.085,0.086\n",
      "Epoch 49. Time,GRU,LSTM: 5.775,6.518,7.048, Train loss,GRU,LSTM: 0.098,0.014,0.013, delta RGU,LSTM:0.084,0.086\n",
      "Epoch 50. Time,GRU,LSTM: 5.754,6.534,7.044, Train loss,GRU,LSTM: 0.093,0.014,0.012, delta RGU,LSTM:0.079,0.081\n",
      "Epoch 51. Time,GRU,LSTM: 5.765,6.535,7.034, Train loss,GRU,LSTM: 0.096,0.014,0.012, delta RGU,LSTM:0.082,0.084\n",
      "Epoch 52. Time,GRU,LSTM: 5.727,6.534,7.046, Train loss,GRU,LSTM: 0.091,0.014,0.012, delta RGU,LSTM:0.077,0.079\n",
      "Epoch 53. Time,GRU,LSTM: 5.744,6.543,7.060, Train loss,GRU,LSTM: 0.090,0.014,0.012, delta RGU,LSTM:0.077,0.079\n",
      "Epoch 54. Time,GRU,LSTM: 5.749,6.520,7.044, Train loss,GRU,LSTM: 0.098,0.014,0.012, delta RGU,LSTM:0.084,0.086\n",
      "Epoch 55. Time,GRU,LSTM: 5.742,6.531,7.044, Train loss,GRU,LSTM: 0.104,0.014,0.012, delta RGU,LSTM:0.090,0.092\n",
      "Epoch 56. Time,GRU,LSTM: 5.740,6.541,7.043, Train loss,GRU,LSTM: 0.103,0.013,0.011, delta RGU,LSTM:0.090,0.092\n",
      "Epoch 57. Time,GRU,LSTM: 5.737,6.526,7.045, Train loss,GRU,LSTM: 0.119,0.013,0.011, delta RGU,LSTM:0.106,0.108\n",
      "Epoch 58. Time,GRU,LSTM: 5.743,6.514,7.050, Train loss,GRU,LSTM: 0.122,0.013,0.011, delta RGU,LSTM:0.109,0.111\n",
      "Epoch 59. Time,GRU,LSTM: 5.746,6.533,7.049, Train loss,GRU,LSTM: 0.114,0.013,0.011, delta RGU,LSTM:0.101,0.103\n",
      "Epoch 60. Time,GRU,LSTM: 5.765,6.531,7.045, Train loss,GRU,LSTM: 0.108,0.013,0.011, delta RGU,LSTM:0.094,0.097\n",
      "Epoch 61. Time,GRU,LSTM: 5.743,6.534,7.052, Train loss,GRU,LSTM: 0.100,0.013,0.011, delta RGU,LSTM:0.087,0.089\n",
      "Epoch 62. Time,GRU,LSTM: 5.745,6.538,7.051, Train loss,GRU,LSTM: 0.107,0.013,0.011, delta RGU,LSTM:0.094,0.096\n",
      "Epoch 63. Time,GRU,LSTM: 5.752,6.544,7.046, Train loss,GRU,LSTM: 0.099,0.013,0.011, delta RGU,LSTM:0.086,0.088\n",
      "Epoch 64. Time,GRU,LSTM: 5.746,6.513,7.028, Train loss,GRU,LSTM: 0.092,0.013,0.011, delta RGU,LSTM:0.079,0.081\n",
      "Epoch 65. Time,GRU,LSTM: 5.747,6.528,7.041, Train loss,GRU,LSTM: 0.088,0.013,0.011, delta RGU,LSTM:0.075,0.077\n",
      "Epoch 66. Time,GRU,LSTM: 5.757,6.533,7.046, Train loss,GRU,LSTM: 0.084,0.013,0.010, delta RGU,LSTM:0.072,0.074\n",
      "Epoch 67. Time,GRU,LSTM: 5.742,6.531,7.052, Train loss,GRU,LSTM: 0.075,0.013,0.010, delta RGU,LSTM:0.062,0.065\n",
      "Epoch 68. Time,GRU,LSTM: 5.764,6.535,7.051, Train loss,GRU,LSTM: 0.075,0.012,0.010, delta RGU,LSTM:0.063,0.065\n",
      "Epoch 69. Time,GRU,LSTM: 5.743,6.540,7.111, Train loss,GRU,LSTM: 0.073,0.012,0.010, delta RGU,LSTM:0.060,0.063\n",
      "Epoch 70. Time,GRU,LSTM: 5.738,6.522,7.032, Train loss,GRU,LSTM: 0.063,0.012,0.010, delta RGU,LSTM:0.050,0.053\n",
      "Epoch 71. Time,GRU,LSTM: 5.739,6.532,7.041, Train loss,GRU,LSTM: 0.050,0.012,0.010, delta RGU,LSTM:0.038,0.040\n",
      "Epoch 72. Time,GRU,LSTM: 5.743,6.531,7.054, Train loss,GRU,LSTM: 0.042,0.012,0.010, delta RGU,LSTM:0.030,0.032\n",
      "Epoch 73. Time,GRU,LSTM: 5.742,6.527,7.050, Train loss,GRU,LSTM: 0.032,0.012,0.010, delta RGU,LSTM:0.020,0.022\n",
      "Epoch 74. Time,GRU,LSTM: 5.754,6.534,7.053, Train loss,GRU,LSTM: 0.025,0.012,0.010, delta RGU,LSTM:0.013,0.015\n",
      "Epoch 75. Time,GRU,LSTM: 5.756,6.543,7.054, Train loss,GRU,LSTM: 0.020,0.012,0.010, delta RGU,LSTM:0.008,0.010\n",
      "Epoch 76. Time,GRU,LSTM: 5.742,6.517,7.052, Train loss,GRU,LSTM: 0.018,0.012,0.010, delta RGU,LSTM:0.006,0.008\n",
      "Epoch 77. Time,GRU,LSTM: 5.735,6.522,7.050, Train loss,GRU,LSTM: 0.017,0.012,0.010, delta RGU,LSTM:0.005,0.007\n",
      "Epoch 78. Time,GRU,LSTM: 5.730,6.530,7.050, Train loss,GRU,LSTM: 0.016,0.012,0.010, delta RGU,LSTM:0.004,0.006\n",
      "Epoch 79. Time,GRU,LSTM: 5.736,6.525,7.027, Train loss,GRU,LSTM: 0.015,0.012,0.010, delta RGU,LSTM:0.003,0.005\n",
      "Epoch 80. Time,GRU,LSTM: 5.752,6.526,7.044, Train loss,GRU,LSTM: 0.015,0.012,0.010, delta RGU,LSTM:0.003,0.005\n",
      "Epoch 81. Time,GRU,LSTM: 5.740,6.526,7.048, Train loss,GRU,LSTM: 0.014,0.012,0.010, delta RGU,LSTM:0.002,0.004\n",
      "Epoch 82. Time,GRU,LSTM: 5.749,6.529,7.044, Train loss,GRU,LSTM: 0.014,0.012,0.010, delta RGU,LSTM:0.002,0.004\n",
      "Epoch 83. Time,GRU,LSTM: 5.744,6.518,7.053, Train loss,GRU,LSTM: 0.014,0.012,0.010, delta RGU,LSTM:0.002,0.004\n",
      "Epoch 84. Time,GRU,LSTM: 5.745,6.539,7.054, Train loss,GRU,LSTM: 0.013,0.012,0.010, delta RGU,LSTM:0.002,0.004\n",
      "Epoch 85. Time,GRU,LSTM: 5.754,6.519,7.039, Train loss,GRU,LSTM: 0.013,0.012,0.010, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 86. Time,GRU,LSTM: 5.747,6.530,7.041, Train loss,GRU,LSTM: 0.013,0.012,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 87. Time,GRU,LSTM: 5.739,6.538,7.047, Train loss,GRU,LSTM: 0.013,0.012,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 88. Time,GRU,LSTM: 5.754,6.526,7.039, Train loss,GRU,LSTM: 0.012,0.012,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 89. Time,GRU,LSTM: 5.742,6.525,7.051, Train loss,GRU,LSTM: 0.012,0.012,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 90. Time,GRU,LSTM: 5.759,6.528,7.044, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 91. Time,GRU,LSTM: 5.737,6.548,7.046, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.001,0.003\n",
      "Epoch 92. Time,GRU,LSTM: 5.742,6.530,7.050, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.000,0.003\n",
      "Epoch 93. Time,GRU,LSTM: 5.758,6.525,7.044, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.000,0.003\n",
      "Epoch 94. Time,GRU,LSTM: 5.746,6.525,7.046, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.000,0.002\n",
      "Epoch 95. Time,GRU,LSTM: 5.747,6.509,7.035, Train loss,GRU,LSTM: 0.012,0.011,0.009, delta RGU,LSTM:0.000,0.002\n",
      "Epoch 96. Time,GRU,LSTM: 5.749,6.527,7.043, Train loss,GRU,LSTM: 0.011,0.011,0.009, delta RGU,LSTM:0.000,0.002\n",
      "Epoch 97. Time,GRU,LSTM: 5.753,6.531,7.043, Train loss,GRU,LSTM: 0.011,0.011,0.009, delta RGU,LSTM:0.000,0.002\n",
      "Epoch 98. Time,GRU,LSTM: 5.755,6.527,7.100, Train loss,GRU,LSTM: 0.011,0.011,0.009, delta RGU,LSTM:0.000,0.002\n",
      "Epoch 99. Time,GRU,LSTM: 5.744,6.541,7.054, Train loss,GRU,LSTM: 0.011,0.011,0.009, delta RGU,LSTM:0.000,0.002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-b6c35f048d4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mY_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0moptimizerLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelLSTM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%debug\n",
    "count = 0\n",
    "\n",
    "for ep in range(500):\n",
    "    \n",
    "    # RNN section\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "    \n",
    "    for i in range(int(len(X_train) / bs)):\n",
    "        batch = X_train[i * bs:(i + 1) * bs]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = Y_train[i * bs:(i + 1) * bs]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        answers, states = model.forward(X_batch)\n",
    "        answers = answers.reshape(bs,2)\n",
    "        loss = criterion(answers, Y_batch)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "    end = time.time()\n",
    "        \n",
    "    # GRU section\n",
    "    startGRU = time.time()\n",
    "    train_lossGRU = 0.\n",
    "    train_passedGRU = 0\n",
    "    \n",
    "    for i in range(int(len(X_train) / bs)):\n",
    "        batch = X_train[i * bs:(i + 1) * bs]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = Y_train[i * bs:(i + 1) * bs]\n",
    "\n",
    "        optimizerGRU.zero_grad()\n",
    "        answers, states = modelGRU.forward(X_batch)\n",
    "        answers = answers.reshape(bs,2)\n",
    "        loss = criterionGRU(answers, Y_batch)\n",
    "        train_lossGRU += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizerGRU.step()\n",
    "        train_passedGRU += 1\n",
    "    endGRU = time.time()\n",
    "    \n",
    "    # LSTM section\n",
    "    startLSTM = time.time()\n",
    "    train_lossLSTM = 0.\n",
    "    train_passedLSTM = 0\n",
    "    \n",
    "    for i in range(int(len(X_train) / bs)):\n",
    "        batch = X_train[i * bs:(i + 1) * bs]\n",
    "        X_batch = batch[:, :-1]\n",
    "        Y_batch = Y_train[i * bs:(i + 1) * bs]\n",
    "\n",
    "        optimizerLSTM.zero_grad()\n",
    "        answers, states = modelLSTM.forward(X_batch)\n",
    "        answers = answers.reshape(bs,2)\n",
    "        loss = criterionLSTM(answers, Y_batch)\n",
    "        train_lossLSTM += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizerLSTM.step()\n",
    "        train_passedLSTM += 1\n",
    "    endLSTM = time.time()\n",
    "    \n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        count = 0\n",
    "        trloss = train_loss / train_passed\n",
    "        trlossGRU = train_lossGRU / train_passedGRU\n",
    "        trlossLSTM = train_lossLSTM / train_passedLSTM\n",
    "        print(\"Epoch {}. Time,GRU,LSTM: {:.3f},{:.3f},{:.3f}, Train loss,GRU,LSTM: {:.3f},{:.3f},{:.3f}, delta RGU,LSTM:{:.3f},{:.3f}\".format(ep, end - start, endGRU - startGRU, endLSTM - startLSTM, trloss, trlossGRU,trlossLSTM, trloss - trlossGRU,trloss - trlossLSTM))\n",
    "        #print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}, \".format(ep, end - start,  trloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test, RNN, GRU, LSTM: 0.676180904522613 0.676180904522613 0.676180904522613\n"
     ]
    }
   ],
   "source": [
    "# check accuracy test\n",
    "count_ok, count_okGRU, count_okLSTM = 0,0,0\n",
    "count_not, count_notGRU, count_notLSTM = 0,0,0\n",
    "func_c = torch.nn.LogSoftmax()\n",
    "\n",
    "for i in range(int(len(X_test) / bs)):\n",
    "    batch = X_test[i * bs:(i + 1) * bs]\n",
    "    X_batch = batch[:, :-1]\n",
    "    Y_batch = Y_test[i * bs:(i + 1) * bs]\n",
    "\n",
    "    answers, states = model.forward(X_batch)\n",
    "    answersGRU, states = model.forward(X_batch)\n",
    "    answersLSTM, states = model.forward(X_batch)\n",
    "    \n",
    "    classes = func_c(answers)\n",
    "    classes = classes.argmax(dim=2)\n",
    "    classesGRU = func_c(answersGRU)\n",
    "    classesGRU = classesGRU.argmax(dim=2)\n",
    "    classesLSTM = func_c(answersLSTM)\n",
    "    classesLSTM = classesLSTM.argmax(dim=2)\n",
    "    \n",
    "    for j in range(bs):\n",
    "        if classes[j] == Y_batch[j]:\n",
    "            count_ok += 1\n",
    "        else:\n",
    "            count_not += 1\n",
    "            \n",
    "        if classesGRU[j] == Y_batch[j]:\n",
    "            count_okGRU += 1\n",
    "        else:\n",
    "            count_notGRU += 1\n",
    "            \n",
    "        if classesLSTM[j] == Y_batch[j]:\n",
    "            count_okLSTM += 1\n",
    "        else:\n",
    "            count_notLSTM += 1\n",
    "            \n",
    "print (\"Accuracy test, RNN, GRU, LSTM:\", count_ok/(count_ok + count_not), count_okGRU/(count_okGRU + count_notGRU), count_okLSTM/(count_okLSTM + count_notLSTM))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train, RNN, GRU, LSTM: 0.8947809762202753 0.8947809762202753 0.8947809762202753\n"
     ]
    }
   ],
   "source": [
    "# check accuracy train\n",
    "count_ok, count_okGRU, count_okLSTM = 0,0,0\n",
    "count_not, count_notGRU, count_notLSTM = 0,0,0\n",
    "func_c = torch.nn.LogSoftmax()\n",
    "\n",
    "for i in range(int(len(X_train) / bs)):\n",
    "    batch = X_train[i * bs:(i + 1) * bs]\n",
    "    X_batch = batch[:, :-1]\n",
    "    Y_batch = Y_train[i * bs:(i + 1) * bs]\n",
    "\n",
    "    answers, states = model.forward(X_batch)\n",
    "    answersGRU, states = model.forward(X_batch)\n",
    "    answersLSTM, states = model.forward(X_batch)\n",
    "    \n",
    "    classes = func_c(answers)\n",
    "    classes = classes.argmax(dim=2)\n",
    "    classesGRU = func_c(answersGRU)\n",
    "    classesGRU = classesGRU.argmax(dim=2)\n",
    "    classesLSTM = func_c(answersLSTM)\n",
    "    classesLSTM = classesLSTM.argmax(dim=2)\n",
    "    \n",
    "    for j in range(bs):\n",
    "        if classes[j] == Y_batch[j]:\n",
    "            count_ok += 1\n",
    "        else:\n",
    "            count_not += 1\n",
    "            \n",
    "        if classesGRU[j] == Y_batch[j]:\n",
    "            count_okGRU += 1\n",
    "        else:\n",
    "            count_notGRU += 1\n",
    "            \n",
    "        if classesLSTM[j] == Y_batch[j]:\n",
    "            count_okLSTM += 1\n",
    "        else:\n",
    "            count_notLSTM += 1\n",
    "            \n",
    "print (\"Accuracy train, RNN, GRU, LSTM:\", count_ok/(count_ok + count_not), count_okGRU/(count_okGRU + count_notGRU), count_okLSTM/(count_okLSTM + count_notLSTM))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
